{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report for Project 5: Dog Analysis\n",
    "Below, I summarize the data wrangling process undertaken for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href='#sources'>Sources</a></li>\n",
    "<li><a href='#gather'>Gathering Data</a></li>\n",
    "<li><a href='#assess'>Assessing Data</a></li>\n",
    "<li><a href='#clean'>Cleaning Data</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sources'></a>\n",
    "## Sources\n",
    "In this project, I worked with the WeRateDogs Twitter account's tweets. This was a combination of the following sources:\n",
    "\n",
    "- Archived Twitter Data\n",
    "- Image Predictions for Tweets\n",
    "- Additional Tweet Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather'></a>\n",
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, I sought to convert each one of the sources above into a Pandas dataframe, which would allow easier, more fluid manipulation during the following stages. This was accomplished by the following steps for each source:\n",
    "#### Archived Twitter Data\n",
    "- Used the pd.read_csv built-in function to conver the on-hand file into a dataframe\n",
    "#### Image Predictions for Tweets\n",
    "- Pulled the file from Udacity's hosting location using the _requests_ library and wrote it to this directory\n",
    "- Read the .tsv file in as a dataframe using pd.read_csv with the separator denoted as tabs\n",
    "#### Additional Tweet Metadata\n",
    "- Setup a twitter developer account\n",
    "- Used the Twitter API via the _tweepy_ library to access data for each tweet from Twitter\n",
    "- Dumped the json data using the _json_ library and wrote to a .txt file in this directory\n",
    "- Read the .txt file in as a dataframe using pd.read_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "## Assessing Data\n",
    "I used a variety of pandas methods such as `df.head()`, `df.sample()`, and indexing to visually assess the data and investigate certain data issues. I complimented that with also using programmatical methods such as `df.info()`, `df.Series.value_counts()`, and `df.query()`. Each issue assessed was then recorded in a numerical list according to their being an issue of _Quality_ or *Tidiness*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='clean'></a>\n",
    "## Cleaning Data\n",
    "In this stage, I addressed each of the problems written in the **Issues** section, breaking the process down into three steps: *Describe*, *Code*, and *Test*. The goal was to have tidy, quality data to work with in the rest of the data analysis process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Notes\n",
    "##### Iteration\n",
    "I iterated a few times due to finding additional issues in the data while performing some cleaning step. For instance, it did not occur to me at first that `source` had any problems at all. It was only later when I was examining the metadata more closely from the API, that I realized this was the _type_ of source of the tweet itself, and was categorical data as there were only a few possible sources.\n",
    "##### Fixes\n",
    "The majority of fixes were accomplished using built-in pandas function such as `.drop()`, `.rename()`, and `.merge()`. They were fixed in an order that made sense for simplicity of code, rather than in the numerical order of the issues as written. For example, the tidiness issue of having three dataframes was fixed early on, so it simplified fixing other issues by having less columns and tables to deal with.\n",
    "##### Dropping Outliers\n",
    "I dropped rows to fix the _Quality_ issues \\#4 and \\#5. These rows were technically contained valid data, but for various reasons the ratings were far outside the majority of the distribution of ratings. They would significantly skew the data and make data visualization less discernible because of scale. Also, fixing these data points would require either complex reprocessing of the tweet text, or manually looking at and fixing each one. With almost 30 rows, this did not seem worth doing in the scope of this project. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dogs]",
   "language": "python",
   "name": "conda-env-dogs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
